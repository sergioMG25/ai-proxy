# AI Proxy Free

Servidor proxy que distribuye solicitudes entre mÃºltiples proveedores de IA (Groq, Cerebras, Gemini) con **balanceo automÃ¡tico** y **fallback inteligente**.

## ğŸŒŸ CaracterÃ­sticas

- ğŸ”„ **Balanceo Round-Robin**: Distribuye solicitudes entre los 3 proveedores
- ğŸ›¡ï¸ **Fallback AutomÃ¡tico**: Si un servicio falla, automÃ¡ticamente usa el siguiente
- ğŸ¥ **Health Monitoring**: Endpoint para verificar estado y estadÃ­sticas
- âœ… **ValidaciÃ³n de Entrada**: Valida requests antes de procesarlos
- ğŸ“Š **Logging Mejorado**: Logs con timestamps y tracking de uso
- âš¡ **Streaming**: Respuestas en tiempo real con Server-Sent Events
- ğŸŒ **CORS**: Habilitado para desarrollo local

## ğŸ“‹ Requisitos

- [Bun](https://bun.sh) instalado en tu sistema
- Claves API de:
  - [Groq](https://console.groq.com)
  - [Cerebras](https://cloud.cerebras.ai)
  - [Google AI](https://makersuite.google.com/app/apikey)

## ğŸš€ InstalaciÃ³n

1. **Instala las dependencias:**

```bash
bun install
```

2. **Configura tus claves API en el archivo `.env`:**

```env
GROQ_API_KEY=tu_clave_real_de_groq
CEREBRAS_API_KEY=tu_clave_real_de_cerebras
GOOGLE_API_KEY=tu_clave_real_de_google
PORT=3001
```

## ğŸ’» Uso

### Iniciar el servidor

```bash
bun run index.ts
```

El servidor iniciarÃ¡ en el puerto 3001 (o el configurado en `.env`).

### Endpoint: POST /chat

EnvÃ­a solicitudes de chat con streaming:

```bash
curl -X POST http://localhost:3001/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hola, Â¿cÃ³mo estÃ¡s?"}
    ]
  }'
```

**Request:**

```json
{
  "messages": [
    { "role": "system", "content": "Eres un asistente Ãºtil" },
    { "role": "user", "content": "Â¿QuÃ© es TypeScript?" }
  ]
}
```

**Response:** Stream de texto con los chunks de la respuesta

### Endpoint: GET /health

Verifica el estado del servidor y obtÃ©n estadÃ­sticas:

```bash
curl http://localhost:3001/health
```

**Response:**

```json
{
  "status": "ok",
  "timestamp": "2025-12-30T06:10:00.000Z",
  "stats": {
    "uptime": "3600s",
    "services": {
      "Groq": { "count": 10, "failures": 1 },
      "Cerebras": { "count": 8, "failures": 0 },
      "Gemini": { "count": 9, "failures": 2 }
    }
  }
}
```

## ğŸ—ï¸ Arquitectura

### Balanceo Round-Robin

El servidor distribuye las solicitudes entre los tres proveedores de IA:

1. Primera solicitud â†’ Groq
2. Segunda solicitud â†’ Cerebras
3. Tercera solicitud â†’ Gemini
4. Cuarta solicitud â†’ Groq (y asÃ­ sucesivamente)

### Fallback AutomÃ¡tico

Si un servicio falla (por API key invÃ¡lida, lÃ­mite de rate, error de red, etc.), el sistema **automÃ¡ticamente** intenta con el siguiente servicio disponible:

```
Request â†’ Groq (falla) â†’ Cerebras (falla) â†’ Gemini (Ã©xito) â†’ Response
```

Esto garantiza alta disponibilidad incluso cuando uno o mÃ¡s servicios estÃ¡n caÃ­dos.

### Modelos Usados

- **Groq**: `mixtral-8x7b-32768`
- **Cerebras**: `llama3-70b-8192`
- **Gemini**: `gemini-1.5-flash`

## ğŸ“ Estructura del Proyecto

```
ai-proxy-free/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ groq.ts       - IntegraciÃ³n con Groq
â”‚   â”œâ”€â”€ cerebras.ts   - IntegraciÃ³n con Cerebras
â”‚   â””â”€â”€ gemini.ts     - IntegraciÃ³n con Gemini
â”œâ”€â”€ types.ts          - Interfaces TypeScript
â”œâ”€â”€ utils.ts          - Utilidades (logging, validaciÃ³n, stats)
â”œâ”€â”€ index.ts          - Servidor principal
â”œâ”€â”€ tsconfig.json     - ConfiguraciÃ³n TypeScript
â”œâ”€â”€ package.json      - Dependencias
â””â”€â”€ .env              - Variables de entorno
```

## ğŸ” Logging

El servidor proporciona logs detallados con timestamps:

```
[2025-12-30T06:10:00.000Z] INFO: ğŸš€ Server running on port 3001
[2025-12-30T06:10:05.123Z] INFO: Processing chat request with 2 messages
[2025-12-30T06:10:05.124Z] INFO: Attempt 1/3: Using service Groq
[2025-12-30T06:10:06.456Z] INFO: Successfully completed request with Groq
```

## âš ï¸ Manejo de Errores

### ValidaciÃ³n de Entrada

El servidor valida:

- Que exista un array `messages`
- Que cada mensaje tenga `role` y `content`
- Que el `role` sea vÃ¡lido: `system`, `user`, o `assistant`

### Respuestas de Error

Todas las respuestas de error son JSON estructurado:

```json
{
  "error": "Request must include 'messages' array"
}
```

## ğŸ³ Docker

### ConstrucciÃ³n de la Imagen

```bash
docker build -t ai-proxy-free .
```

### Ejecutar con Docker

```bash
docker run -d \
  -p 3001:3001 \
  -e GROQ_API_KEY=tu_clave_groq \
  -e CEREBRAS_API_KEY=tu_clave_cerebras \
  -e GOOGLE_API_KEY=tu_clave_google \
  --name ai-proxy \
  ai-proxy-free
```

### Ejecutar con Docker Compose

1. AsegÃºrate de tener tus claves API en el archivo `.env`

2. Inicia el servicio:

```bash
docker-compose up -d
```

3. Ver logs:

```bash
docker-compose logs -f
```

4. Detener el servicio:

```bash
docker-compose down
```

### Health Check

Docker incluye health checks automÃ¡ticos que verifican el endpoint `/health` cada 30 segundos.

## ğŸ”§ Desarrollo

### Requisitos de TypeScript

El proyecto usa TypeScript con configuraciÃ³n estricta. Los tipos globales de Bun estÃ¡n incluidos.

### Environment Variables

Si faltan variables de entorno, el servidor mostrarÃ¡ warnings pero continuarÃ¡ ejecutÃ¡ndose (los servicios sin API key fallarÃ¡n cuando se usen).

## ğŸ“ Notas

- El servidor usa CORS abierto (`*`) para facilitar el desarrollo local
- Las respuestas se envÃ­an en formato Server-Sent Events (SSE)
- El fallback automÃ¡tico mejora significativamente la disponibilidad
- Las estadÃ­sticas se reinician cuando el servidor se reinicia

## ğŸ› Troubleshooting

**Problema**: "All services failed"

- **SoluciÃ³n**: Verifica que al menos una API key sea vÃ¡lida en `.env`

**Problema**: Errores de TypeScript

- **SoluciÃ³n**: Ejecuta `bun install` para instalar las dependencias

**Problema**: El servidor no inicia

- **SoluciÃ³n**: AsegÃºrate de tener Bun instalado y que el puerto 3001 estÃ© disponible
